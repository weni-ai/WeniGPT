{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uMafmwKtiQt"
      },
      "outputs": [],
      "source": [
        "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" sentencepiece fire einops --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_instruction(sample):\n",
        "\treturn f\"\"\"### Instruction:\n",
        "Responda à pergunta com a maior sinceridade possível usando o CONTEXTO fornecido e, se a resposta não estiver contida no CONTEXTO abaixo, diga 'Desculpe, não possuo essa informação'.\n",
        "\n",
        "### Input:\n",
        "CONTEXTO: {sample['context']}\n",
        "\n",
        "PERGUNTA: {sample['question']}\n",
        "\n",
        "### Response: {sample['resposta']}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uRXfG2mHtmQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from random import randrange\n",
        "\n",
        "dataset = load_dataset(\"Weni/LLM-base\", split='train')\n",
        "dataset = dataset.shuffle(seed=55)"
      ],
      "metadata": {
        "id": "Urgs5rzctmvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"upstage/Llama-2-70b-instruct-v2\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, use_cache=False, device_map=\"auto\", use_safetensors=False)\n",
        "model.config.pretraining_tp = 1\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "-pOPhFJBtqv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        r=64,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "vptsk9Apt8hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"upstage-70\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=6e-4,\n",
        "    bf16=True,\n",
        "    tf32=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    disable_tqdm=True\n",
        ")"
      ],
      "metadata": {
        "id": "WPhlINHHuAxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "max_seq_length = 2048\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset.with_format(\"torch\"),\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=True,\n",
        "    formatting_func=format_instruction,\n",
        "    args=args,\n",
        ")"
      ],
      "metadata": {
        "id": "rfTvFSW6uHGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "Lg9GA49QuLS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = 'upstage-70-files'\n",
        "base_model_name_or_path = 'upstage/Llama-2-70b-instruct-v2'\n",
        "device_arg = { 'device_map': 'auto' }\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name_or_path,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    **device_arg\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, 'upstage-70', **device_arg)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "model.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "A5v3xz1uuT9s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}